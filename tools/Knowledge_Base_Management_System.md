# çŸ¥è¯†åº“æ–‡æ¡£ç®¡ç†ç³»ç»Ÿä¸è‡ªåŠ¨åŒ–æ£€æŸ¥å·¥å…·

## ğŸ“‹ ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
- **æ–‡æ¡£ç»“æ„ç®¡ç†** - è‡ªåŠ¨åŒ–ç›®å½•ç»„ç»‡å’Œæ–‡ä»¶åˆ†ç±»
- **è´¨é‡æ£€æŸ¥ç³»ç»Ÿ** - å†…å®¹å®Œæ•´æ€§ã€æ ¼å¼è§„èŒƒæ€§è‡ªåŠ¨æ£€æµ‹
- **ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ** - æ–‡æ¡£å˜æ›´è¿½è¸ªå’Œå†å²ç‰ˆæœ¬ç®¡ç†
- **ç´¢å¼•æœç´¢å¼•æ“** - æ™ºèƒ½å†…å®¹æ£€ç´¢å’Œå…³è”æ¨è
- **ç»Ÿè®¡åˆ†æé¢æ¿** - çŸ¥è¯†åº“å¥åº·åº¦å’Œä½¿ç”¨æƒ…å†µç›‘æ§

### æŠ€æœ¯æ ˆé€‰å‹
```yaml
åç«¯æ¡†æ¶: Python Flask/Django
å‰ç«¯ç•Œé¢: React/Vue.js æˆ–çº¯HTML/CSS
æ•°æ®åº“: SQLite/PostgreSQL (è½»é‡çº§éƒ¨ç½²)
æœç´¢å¼•æ“: Whoosh/Elasticsearch
æ–‡ä»¶å¤„ç†: Pythonæ ‡å‡†åº“ + Markdownè§£æå™¨
éƒ¨ç½²æ–¹å¼: Dockerå®¹å™¨åŒ–æˆ–ç›´æ¥Pythonè¿è¡Œ
```

## ğŸ› ï¸ æ ¸å¿ƒå·¥å…·å®ç°

### 1. æ–‡æ¡£ç»“æ„åˆ†æå™¨ (doc_analyzer.py)

```python
#!/usr/bin/env python3
"""
çŸ¥è¯†åº“æ–‡æ¡£ç»“æ„åˆ†æå™¨
åŠŸèƒ½ï¼šæ‰«æç›®å½•ç»“æ„ï¼Œç”Ÿæˆæ–‡æ¡£æ¸…å•ï¼Œè¯†åˆ«ç¼ºå¤±å†…å®¹
"""

import os
import json
import yaml
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime
import re

class DocumentAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.document_types = {
            'overview': ['Overview.md', 'æ€»è§ˆ.md', 'ç®€ä»‹.md'],
            'treatment': ['Treatment.md', 'æ²»ç–—.md', 'å¹²é¢„.md'],
            'assessment': ['Assessment.md', 'è¯„ä¼°.md', 'è¯Šæ–­.md'],
            'research': ['Research.md', 'ç ”ç©¶.md', 'æ–‡çŒ®.md'],
            'case': ['Case.md', 'æ¡ˆä¾‹.md', 'å®ä¾‹.md']
        }
        self.required_sections = [
            'æ ¸å¿ƒæ¦‚å¿µ', 'ç†è®ºåŸºç¡€', 'ä¸´åºŠåº”ç”¨', 'ç ”ç©¶è¯æ®',
            'å®æ–½æ–¹æ³•', 'æ³¨æ„äº‹é¡¹', 'å‚è€ƒæ–‡çŒ®'
        ]
        
    def scan_directory(self) -> Dict:
        """æ‰«ææ•´ä¸ªçŸ¥è¯†åº“ç›®å½•ç»“æ„"""
        structure = {}
        stats = {
            'total_dirs': 0,
            'total_files': 0,
            'markdown_files': 0,
            'missing_overviews': [],
            'incomplete_docs': []
        }
        
        for root, dirs, files in os.walk(self.base_path):
            # è·³è¿‡éšè—ç›®å½•å’Œç³»ç»Ÿç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
            
            rel_path = Path(root).relative_to(self.base_path)
            if str(rel_path) == '.':
                continue
                
            current_level = structure
            for part in rel_path.parts[:-1]:
                current_level = current_level.setdefault(part, {})
            
            dir_name = rel_path.parts[-1]
            current_level[dir_name] = {
                'files': [],
                'subdirs': {},
                'metadata': self._extract_metadata(str(rel_path))
            }
            
            # å¤„ç†æ–‡ä»¶
            md_files = [f for f in files if f.endswith('.md')]
            stats['total_dirs'] += 1
            stats['total_files'] += len(files)
            stats['markdown_files'] += len(md_files)
            
            current_level[dir_name]['files'] = md_files
            
            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘æ¦‚è§ˆæ–‡æ¡£
            if not any(overview in md_files for overview in self.document_types['overview']):
                stats['missing_overviews'].append(str(rel_path))
            
            # æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§
            for md_file in md_files:
                file_path = Path(root) / md_file
                if not self._check_document_completeness(file_path):
                    stats['incomplete_docs'].append(str(file_path.relative_to(self.base_path)))
        
        return {
            'structure': structure,
            'statistics': stats,
            'timestamp': datetime.now().isoformat()
        }
    
    def _extract_metadata(self, dir_path: str) -> Dict:
        """æå–ç›®å½•å…ƒæ•°æ®"""
        metadata = {'category': 'unknown', 'priority': 'medium'}
        
        # æ ¹æ®ç›®å½•åç§°æ¨æ–­ç±»åˆ«
        path_parts = Path(dir_path).parts
        if path_parts:
            last_part = path_parts[-1].lower()
            if any(keyword in last_part for keyword in ['therapy', 'æ²»ç–—']):
                metadata['category'] = 'treatment'
            elif any(keyword in last_part for keyword in ['assessment', 'è¯„ä¼°']):
                metadata['category'] = 'assessment'
            elif any(keyword in last_part for keyword in ['research', 'ç ”ç©¶']):
                metadata['category'] = 'research'
                
        return metadata
    
    def _check_document_completeness(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥å¿…éœ€ç« èŠ‚æ˜¯å¦å­˜åœ¨
            missing_sections = []
            for section in self.required_sections:
                if not re.search(rf'[#\*]*\s*{section}', content, re.IGNORECASE):
                    missing_sections.append(section)
            
            # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
            if len(content.strip()) < 500:
                return False
                
            # æ£€æŸ¥åŸºæœ¬æ ¼å¼
            if not (content.startswith('#') or content.startswith('---')):
                return False
                
            return len(missing_sections) <= 2  # å…è®¸æœ€å¤šç¼ºå°‘2ä¸ªç« èŠ‚
            
        except Exception:
            return False
    
    def generate_report(self, output_path: str = "knowledge_base_analysis.json"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis_result = self.scan_directory()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        
        return analysis_result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = DocumentAnalyzer(".")
    result = analyzer.generate_report("knowledge_base_analysis.json")
    print(f"åˆ†æå®Œæˆï¼å…±å‘ç° {result['statistics']['total_dirs']} ä¸ªä¸»é¢˜ç›®å½•")
    print(f"ç¼ºå¤±æ¦‚è§ˆæ–‡æ¡£çš„ç›®å½•: {len(result['statistics']['missing_overviews'])} ä¸ª")
    print(f"ä¸å®Œæ•´æ–‡æ¡£æ•°é‡: {len(result['statistics']['incomplete_docs'])} ä¸ª")
```

### 2. è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥å™¨ (quality_checker.py)

```python
#!/usr/bin/env python3
"""
æ–‡æ¡£è´¨é‡è‡ªåŠ¨åŒ–æ£€æŸ¥å™¨
åŠŸèƒ½ï¼šæ£€æŸ¥æ–‡æ¡£æ ¼å¼ã€å†…å®¹å®Œæ•´æ€§ã€å¼•ç”¨è§„èŒƒæ€§ç­‰
"""

import os
import re
import yaml
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

class QualityChecker:
    def __init__(self):
        self.check_functions = [
            self._check_title_format,
            self._check_section_structure,
            self._check_citations_format,
            self._check_links_validity,
            self._check_chinese_english_balance,
            self._check_metadata_completeness
        ]
        
    def check_single_document(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å•ä¸ªæ–‡æ¡£çš„è´¨é‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {
                'file': str(file_path),
                'status': 'error',
                'errors': [f'æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}']
            }
        
        results = {
            'file': str(file_path),
            'status': 'pass',
            'checks': {},
            'errors': [],
            'warnings': [],
            'score': 100
        }
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        total_deductions = 0
        for check_func in self.check_functions:
            check_name = check_func.__name__[6:]  # ç§»é™¤ '_check_' å‰ç¼€
            try:
                result = check_func(content, file_path)
                results['checks'][check_name] = result
                
                if result['status'] == 'fail':
                    results['errors'].extend([f"{check_name}: {err}" for err in result.get('errors', [])])
                    total_deductions += result.get('deduction', 10)
                elif result['status'] == 'warning':
                    results['warnings'].extend([f"{check_name}: {warn}" for warn in result.get('warnings', [])])
                    total_deductions += result.get('deduction', 5)
                    
            except Exception as e:
                results['errors'].append(f"{check_name}: æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ - {str(e)}")
                total_deductions += 15
        
        results['score'] = max(0, 100 - total_deductions)
        results['status'] = 'pass' if results['score'] >= 80 else 'fail'
        
        return results
    
    def _check_title_format(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥æ ‡é¢˜æ ¼å¼"""
        lines = content.split('\n')
        first_line = lines[0].strip() if lines else ""
        
        if not first_line.startswith('#'):
            return {
                'status': 'fail',
                'errors': ['æ–‡æ¡£å¿…é¡»ä»¥H1æ ‡é¢˜å¼€å¤´'],
                'deduction': 15
            }
        
        if len(first_line) < 10:
            return {
                'status': 'warning',
                'warnings': ['æ ‡é¢˜è¿‡äºç®€çŸ­ï¼Œå»ºè®®æ›´å…·ä½“'],
                'deduction': 5
            }
        
        return {'status': 'pass'}
    
    def _check_section_structure(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥ç« èŠ‚ç»“æ„å®Œæ•´æ€§"""
        required_sections = ['æ ¸å¿ƒæ¦‚å¿µ', 'ç†è®ºåŸºç¡€', 'ä¸´åºŠåº”ç”¨', 'ç ”ç©¶è¯æ®']
        missing_sections = []
        
        for section in required_sections:
            if not re.search(rf'[#\*]+\s*{section}', content):
                missing_sections.append(section)
        
        if missing_sections:
            return {
                'status': 'warning' if len(missing_sections) <= 2 else 'fail',
                'warnings' if len(missing_sections) <= 2 else 'errors': 
                    [f'ç¼ºå°‘å¿…è¦ç« èŠ‚: {", ".join(missing_sections)}'],
                'deduction': len(missing_sections) * 8
            }
        
        return {'status': 'pass'}
    
    def _check_citations_format(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥å¼•ç”¨æ ¼å¼è§„èŒƒæ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ ¼å¼åŒ–çš„å¼•ç”¨
        unformatted_refs = re.findall(r'\([^)]*(doi|DOI)[^)]*\)', content)
        if unformatted_refs:
            return {
                'status': 'warning',
                'warnings': [f'å‘ç° {len(unformatted_refs)} ä¸ªæœªè§„èŒƒæ ¼å¼çš„å¼•ç”¨'],
                'deduction': min(len(unformatted_refs) * 3, 15)
            }
        
        return {'status': 'pass'}
    
    def _check_links_validity(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥å†…éƒ¨é“¾æ¥
        internal_links = re.findall(r'\[.*?\]\(([^)]+)\)', content)
        broken_links = []
        
        for link in internal_links:
            if link.startswith(('http://', 'https://')):
                continue  # å¤–éƒ¨é“¾æ¥æš‚ä¸æ£€æŸ¥
            
            # æ£€æŸ¥ç›¸å¯¹è·¯å¾„é“¾æ¥
            if link.startswith('./') or link.startswith('../'):
                target_path = file_path.parent / link
                if not target_path.exists():
                    broken_links.append(link)
        
        if broken_links:
            return {
                'status': 'warning',
                'warnings': [f'å‘ç°æ— æ•ˆå†…éƒ¨é“¾æ¥: {", ".join(broken_links[:3])}'],
                'deduction': min(len(broken_links) * 2, 10)
            }
        
        return {'status': 'pass'}
    
    def _check_chinese_english_balance(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥ä¸­è‹±æ–‡æ··æ’å¹³è¡¡æ€§"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        english_chars = len(re.findall(r'[a-zA-Z]', content))
        
        if chinese_chars > 0 and english_chars > 0:
            ratio = english_chars / (chinese_chars + english_chars)
            if ratio > 0.3:  # è‹±æ–‡æ¯”ä¾‹è¿‡é«˜
                return {
                    'status': 'warning',
                    'warnings': [f'è‹±æ–‡å­—ç¬¦å æ¯” {ratio:.1%} è¿‡é«˜ï¼Œå»ºè®®å¢åŠ ä¸­æ–‡å†…å®¹'],
                    'deduction': 5
                }
        
        return {'status': 'pass'}
    
    def _check_metadata_completeness(self, content: str, file_path: Path) -> Dict:
        """æ£€æŸ¥æ–‡æ¡£å…ƒæ•°æ®å®Œæ•´æ€§"""
        # æ£€æŸ¥YAML front matter
        yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
        if not yaml_match:
            return {
                'status': 'warning',
                'warnings': ['ç¼ºå°‘YAMLå…ƒæ•°æ®å—'],
                'deduction': 8
            }
        
        try:
            metadata = yaml.safe_load(yaml_match.group(1))
            required_fields = ['title', 'author', 'date', 'tags']
            missing_fields = [field for field in required_fields if field not in metadata]
            
            if missing_fields:
                return {
                    'status': 'warning',
                    'warnings': [f'å…ƒæ•°æ®ç¼ºå°‘å­—æ®µ: {", ".join(missing_fields)}'],
                    'deduction': len(missing_fields) * 3
                }
                
        except yaml.YAMLError:
            return {
                'status': 'warning',
                'warnings': ['YAMLå…ƒæ•°æ®æ ¼å¼é”™è¯¯'],
                'deduction': 10
            }
        
        return {'status': 'pass'}

# æ‰¹é‡æ£€æŸ¥å·¥å…·
def batch_check_quality(base_path: str = ".", output_file: str = "quality_report.json"):
    """æ‰¹é‡æ£€æŸ¥æ•´ä¸ªçŸ¥è¯†åº“çš„æ–‡æ¡£è´¨é‡"""
    checker = QualityChecker()
    base_path = Path(base_path)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'total_documents': 0,
        'passed_documents': 0,
        'failed_documents': 0,
        'average_score': 0,
        'detailed_results': []
    }
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = list(base_path.rglob("*.md"))
    results['total_documents'] = len(md_files)
    
    scores = []
    for md_file in md_files:
        result = checker.check_single_document(md_file)
        results['detailed_results'].append(result)
        
        scores.append(result['score'])
        if result['status'] == 'pass':
            results['passed_documents'] += 1
        else:
            results['failed_documents'] += 1
    
    if scores:
        results['average_score'] = sum(scores) / len(scores)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return results

if __name__ == "__main__":
    import json
    results = batch_check_quality(".", "quality_report.json")
    print(f"è´¨é‡æ£€æŸ¥å®Œæˆï¼")
    print(f"æ€»æ–‡æ¡£æ•°: {results['total_documents']}")
    print(f"é€šè¿‡ç‡: {results['passed_documents']/results['total_documents']*100:.1f}%")
    print(f"å¹³å‡åˆ†æ•°: {results['average_score']:.1f}")
```

### 3. æ™ºèƒ½ç´¢å¼•æ„å»ºå™¨ (index_builder.py)

```python
#!/usr/bin/env python3
"""
æ™ºèƒ½ç´¢å¼•æ„å»ºå™¨
åŠŸèƒ½ï¼šä¸ºçŸ¥è¯†åº“å»ºç«‹å…¨æ–‡ç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿæœç´¢å’Œå…³è”æ¨è
"""

import os
import json
import re
from pathlib import Path
from whoosh.index import create_in, open_dir
from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED
from whoosh.qparser import QueryParser
from whoosh.analysis import StemmingAnalyzer
from datetime import datetime

class KnowledgeIndexer:
    def __init__(self, index_dir: str = "index"):
        self.index_dir = Path(index_dir)
        self.schema = Schema(
            path=ID(stored=True),
            title=TEXT(stored=True, analyzer=StemmingAnalyzer()),
            content=TEXT(stored=True, analyzer=StemmingAnalyzer()),
            keywords=KEYWORD(stored=True, commas=True),
            category=TEXT(stored=True),
            date=STORED,
            quality_score=STORED
        )
        
    def build_index(self, knowledge_base_path: str = ".") -> dict:
        """æ„å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        # åˆ›å»ºç´¢å¼•ç›®å½•
        if not self.index_dir.exists():
            self.index_dir.mkdir(parents=True)
            ix = create_in(str(self.index_dir), self.schema)
        else:
            ix = open_dir(str(self.index_dir))
        
        writer = ix.writer()
        stats = {
            'indexed_documents': 0,
            'skipped_documents': 0,
            'processing_errors': 0,
            'categories': {}
        }
        
        # éå†æ‰€æœ‰Markdownæ–‡ä»¶
        base_path = Path(knowledge_base_path)
        for md_file in base_path.rglob("*.md"):
            try:
                doc_data = self._process_document(md_file, base_path)
                if doc_data:
                    writer.add_document(**doc_data)
                    stats['indexed_documents'] += 1
                    
                    # ç»Ÿè®¡åˆ†ç±»ä¿¡æ¯
                    category = doc_data.get('category', 'uncategorized')
                    stats['categories'][category] = stats['categories'].get(category, 0) + 1
                else:
                    stats['skipped_documents'] += 1
                    
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {md_file} æ—¶å‡ºé”™: {e}")
                stats['processing_errors'] += 1
        
        writer.commit()
        stats['timestamp'] = datetime.now().isoformat()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        with open(self.index_dir / "index_stats.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        return stats
    
    def _process_document(self, file_path: Path, base_path: Path) -> dict:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–åŸºæœ¬ä¿¡æ¯
            lines = content.split('\n')
            title = self._extract_title(lines)
            keywords = self._extract_keywords(content)
            category = self._extract_category(str(file_path.relative_to(base_path)))
            quality_score = self._calculate_quality_score(content)
            
            return {
                'path': str(file_path.relative_to(base_path)),
                'title': title,
                'content': content,
                'keywords': ','.join(keywords),
                'category': category,
                'date': datetime.now().isoformat(),
                'quality_score': quality_score
            }
        except Exception:
            return None
    
    def _extract_title(self, lines: list) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        for line in lines[:5]:  # æ£€æŸ¥å‰5è¡Œ
            if line.strip().startswith('#'):
                return line.strip('# ').strip()
        return "æœªå‘½åæ–‡æ¡£"
    
    def _extract_keywords(self, content: str) -> list:
        """æå–å…³é”®è¯"""
        # æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ4å­—ä»¥ä¸Šè¯è¯­ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{4,}', content)
        
        # æå–è‹±æ–‡å…³é”®è¯
        english_words = re.findall(r'\b[a-zA-Z]{4,}\b', content)
        
        # åˆå¹¶å¹¶å»é‡
        all_keywords = list(set(chinese_words + english_words))
        return all_keywords[:20]  # æœ€å¤šè¿”å›20ä¸ªå…³é”®è¯
    
    def _extract_category(self, relative_path: str) -> str:
        """ä»è·¯å¾„æå–åˆ†ç±»"""
        path_parts = Path(relative_path).parts
        if len(path_parts) > 1:
            return path_parts[0]
        return "root"
    
    def _calculate_quality_score(self, content: str) -> int:
        """è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°"""
        score = 50  # åŸºç¡€åˆ†
        
        # å†…å®¹é•¿åº¦åŠ åˆ†
        if len(content) > 1000:
            score += 20
        elif len(content) > 500:
            score += 10
            
        # æ ‡é¢˜æ ¼å¼åŠ åˆ†
        if content.strip().startswith('#'):
            score += 10
            
        # ç« èŠ‚ç»“æ„åŠ åˆ†
        section_count = len(re.findall(r'^#+\s', content, re.MULTILINE))
        if section_count >= 3:
            score += 10
        elif section_count >= 2:
            score += 5
            
        # å¼•ç”¨æ ¼å¼åŠ åˆ†
        citation_count = len(re.findall(r'\[\d+\]', content))
        if citation_count >= 3:
            score += 10
            
        return min(100, score)
    
    def search(self, query_string: str, category: str = None, min_score: int = 0) -> list:
        """æœç´¢æ–‡æ¡£"""
        if not self.index_dir.exists():
            raise Exception("ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»ºç´¢å¼•")
        
        ix = open_dir(str(self.index_dir))
        searcher = ix.searcher()
        
        # æ„å»ºæŸ¥è¯¢
        parser = QueryParser("content", schema=self.schema)
        query = parser.parse(query_string)
        
        # æ‰§è¡Œæœç´¢
        results = searcher.search(query, limit=50)
        
        # è¿‡æ»¤ç»“æœ
        filtered_results = []
        for hit in results:
            if hit['quality_score'] >= min_score:
                if category is None or hit['category'] == category:
                    filtered_results.append(dict(hit))
        
        searcher.close()
        return filtered_results

# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='çŸ¥è¯†åº“ç´¢å¼•å·¥å…·')
    parser.add_argument('--build', action='store_true', help='æ„å»ºç´¢å¼•')
    parser.add_argument('--search', type=str, help='æœç´¢å…³é”®è¯')
    parser.add_argument('--path', default='.', help='çŸ¥è¯†åº“è·¯å¾„')
    parser.add_argument('--category', help='é™å®šåˆ†ç±»')
    
    args = parser.parse_args()
    
    indexer = KnowledgeIndexer()
    
    if args.build:
        stats = indexer.build_index(args.path)
        print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼å…±ç´¢å¼• {stats['indexed_documents']} ä¸ªæ–‡æ¡£")
        print(f"åˆ†ç±»ç»Ÿè®¡: {stats['categories']}")
    
    if args.search:
        results = indexer.search(args.search, args.category)
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")
        for i, result in enumerate(results[:10], 1):
            print(f"{i}. {result['title']} ({result['category']}) - è´¨é‡åˆ†: {result['quality_score']}")
```

### 4. çŸ¥è¯†åº“å¥åº·åº¦ä»ªè¡¨æ¿ (dashboard.py)

```python
#!/usr/bin/env python3
"""
çŸ¥è¯†åº“å¥åº·åº¦ç›‘æ§ä»ªè¡¨æ¿
åŠŸèƒ½ï¼šå¯è§†åŒ–å±•ç¤ºçŸ¥è¯†åº“çŠ¶æ€ã€è´¨é‡æŒ‡æ ‡å’Œæ”¹è¿›å»ºè®®
"""

import json
import os
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from jinja2 import Template

class HealthDashboard:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.reports_dir = self.base_path / "reports"
        if not self.reports_dir.exists():
            self.reports_dir.mkdir()
    
    def generate_health_report(self) -> dict:
        """ç”Ÿæˆå¥åº·åº¦æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': 0,
            'metrics': {},
            'recommendations': [],
            'trends': {}
        }
        
        # æ”¶é›†å„é¡¹æŒ‡æ ‡
        metrics = self._collect_metrics()
        report['metrics'] = metrics
        
        # è®¡ç®—æ€»ä½“å¥åº·åº¦åˆ†æ•°
        report['overall_score'] = self._calculate_overall_score(metrics)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        report['recommendations'] = self._generate_recommendations(metrics)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"health_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report
    
    def _collect_metrics(self) -> dict:
        """æ”¶é›†å„é¡¹å¥åº·æŒ‡æ ‡"""
        metrics = {
            'completeness': self._calculate_completeness(),
            'quality': self._calculate_quality(),
            'consistency': self._calculate_consistency(),
            'growth': self._calculate_growth(),
            'usage': self._calculate_usage()
        }
        return metrics
    
    def _calculate_completeness(self) -> float:
        """è®¡ç®—å†…å®¹å®Œæ•´æ€§"""
        # æ£€æŸ¥ç›®å½•ç»“æ„å®Œæ•´æ€§
        analyzer = DocumentAnalyzer(str(self.base_path))
        analysis = analyzer.scan_directory()
        
        total_dirs = analysis['statistics']['total_dirs']
        missing_overviews = len(analysis['statistics']['missing_overviews'])
        
        if total_dirs == 0:
            return 0
        
        completeness = ((total_dirs - missing_overviews) / total_dirs) * 100
        return round(completeness, 1)
    
    def _calculate_quality(self) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡"""
        try:
            with open("quality_report.json", 'r', encoding='utf-8') as f:
                quality_data = json.load(f)
            
            if quality_data['total_documents'] > 0:
                avg_score = quality_data['average_score']
                pass_rate = (quality_data['passed_documents'] / quality_data['total_documents']) * 100
                return round((avg_score + pass_rate) / 2, 1)
        except FileNotFoundError:
            pass
        
        return 70.0  # é»˜è®¤åˆ†æ•°
    
    def _calculate_consistency(self) -> float:
        """è®¡ç®—æ ¼å¼ä¸€è‡´æ€§"""
        # æ£€æŸ¥æ–‡æ¡£æ ¼å¼ç»Ÿä¸€æ€§
        consistent_docs = 0
        total_docs = 0
        
        for md_file in self.base_path.rglob("*.md"):
            total_docs += 1
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥åŸºæœ¬æ ¼å¼è¦æ±‚
                if (content.strip().startswith('#') and 
                    len(re.findall(r'^#+\s', content, re.MULTILINE)) >= 2):
                    consistent_docs += 1
            except:
                pass
        
        if total_docs == 0:
            return 0
        
        consistency = (consistent_docs / total_docs) * 100
        return round(consistency, 1)
    
    def _calculate_growth(self) -> dict:
        """è®¡ç®—å¢é•¿è¶‹åŠ¿"""
        # ç®€å•çš„å¢é•¿æŒ‡æ ‡ï¼ˆå¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„è¶‹åŠ¿åˆ†æï¼‰
        try:
            # è·å–æœ€è¿‘å‡ æ¬¡çš„ç»Ÿè®¡æ•°æ®
            health_reports = sorted(self.reports_dir.glob("health_report_*.json"))
            if len(health_reports) >= 2:
                with open(health_reports[-2], 'r', encoding='utf-8') as f:
                    prev_report = json.load(f)
                with open(health_reports[-1], 'r', encoding='utf-8') as f:
                    curr_report = json.load(f)
                
                growth = {
                    'completeness_change': curr_report['metrics']['completeness'] - prev_report['metrics']['completeness'],
                    'quality_change': curr_report['metrics']['quality'] - prev_report['metrics']['quality'],
                    'last_updated': curr_report['timestamp']
                }
                return growth
        except:
            pass
        
        return {'completeness_change': 0, 'quality_change': 0, 'last_updated': 'N/A'}
    
    def _calculate_usage(self) -> dict:
        """è®¡ç®—ä½¿ç”¨æƒ…å†µï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰"""
        return {
            'monthly_views': 1250,
            'active_contributors': 8,
            'recent_updates': 15
        }
    
    def _calculate_overall_score(self, metrics: dict) -> float:
        """è®¡ç®—æ€»ä½“å¥åº·åº¦åˆ†æ•°"""
        weights = {
            'completeness': 0.25,
            'quality': 0.30,
            'consistency': 0.20,
            'growth': 0.15,
            'usage': 0.10
        }
        
        # æ ‡å‡†åŒ–å„é¡¹æŒ‡æ ‡åˆ°0-100èŒƒå›´
        normalized_metrics = {
            'completeness': metrics['completeness'],
            'quality': metrics['quality'],
            'consistency': metrics['consistency'],
            'growth': min(100, max(0, 50 + metrics['growth'].get('quality_change', 0) * 10)),
            'usage': min(100, metrics['usage']['monthly_views'] / 20)  # ç®€å•æ ‡å‡†åŒ–
        }
        
        score = sum(normalized_metrics[k] * weights[k] for k in weights)
        return round(score, 1)
    
    def _generate_recommendations(self, metrics: dict) -> list:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if metrics['completeness'] < 80:
            recommendations.append("å»ºè®®è¡¥å……ç¼ºå¤±çš„ä¸»é¢˜æ¦‚è§ˆæ–‡æ¡£")
        
        if metrics['quality'] < 75:
            recommendations.append("éœ€è¦æé«˜æ–‡æ¡£è´¨é‡ï¼Œé‡ç‚¹å…³æ³¨æ ¼å¼è§„èŒƒå’Œå†…å®¹å®Œæ•´æ€§")
        
        if metrics['consistency'] < 85:
            recommendations.append("åŠ å¼ºæ–‡æ¡£æ ¼å¼æ ‡å‡†åŒ–ï¼Œç»Ÿä¸€å†™ä½œè§„èŒƒ")
        
        if metrics['growth']['completeness_change'] < 0:
            recommendations.append("æ³¨æ„å†…å®¹å®Œæ•´æ€§ä¸‹é™è¶‹åŠ¿ï¼ŒåŠæ—¶è¡¥å……å®Œå–„")
        
        if len(recommendations) == 0:
            recommendations.append("çŸ¥è¯†åº“å¥åº·çŠ¶å†µè‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        return recommendations
    
    def generate_html_report(self, report_data: dict) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        template_str = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>çŸ¥è¯†åº“å¥åº·åº¦æŠ¥å‘Š</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }
        .metric-card { border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; }
        .score-high { color: green; }
        .score-medium { color: orange; }
        .score-low { color: red; }
        .progress-bar { width: 100%; background: #eee; height: 20px; border-radius: 10px; overflow: hidden; }
        .progress-fill { height: 100%; background: linear-gradient(to right, #ff4444, #ffaa00, #00aa00); }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ§  çŸ¥è¯†åº“å¥åº·åº¦æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {{ timestamp }}</p>
        <h2>æ€»ä½“å¥åº·åº¦: <span class="{{ 'score-high' if overall_score >= 80 else 'score-medium' if overall_score >= 60 else 'score-low' }}">{{ overall_score }}/100</span></h2>
    </div>
    
    <h3>ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡</h3>
    {% for metric_name, value in metrics.items() %}
    <div class="metric-card">
        <h4>{{ metric_name }}: {{ "%.1f"|format(value) if value is number else value }}</h4>
        <div class="progress-bar">
            <div class="progress-fill" style="width: {{ [value, 100]|min }}%"></div>
        </div>
    </div>
    {% endfor %}
    
    <h3>ğŸ’¡ æ”¹è¿›å»ºè®®</h3>
    <ul>
    {% for rec in recommendations %}
        <li>{{ rec }}</li>
    {% endfor %}
    </ul>
    
    <h3>ğŸ“ˆ è¶‹åŠ¿åˆ†æ</h3>
    <p>å®Œæ•´æ€§å˜åŒ–: {{ trends.completeness_change }}%</p>
    <p>è´¨é‡å˜åŒ–: {{ trends.quality_change }}%</p>
</body>
</html>
        """
        
        template = Template(template_str)
        html_content = template.render(**report_data)
        
        html_file = self.reports_dir / f"health_dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(html_file)

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    dashboard = HealthDashboard(".")
    report = dashboard.generate_health_report()
    html_file = dashboard.generate_html_report(report)
    
    print(f"å¥åº·åº¦æŠ¥å‘Šå·²ç”Ÿæˆï¼")
    print(f"æ€»ä½“åˆ†æ•°: {report['overall_score']}/100")
    print(f"HTMLæŠ¥å‘Š: {html_file}")
    print("\nä¸»è¦å»ºè®®:")
    for rec in report['recommendations']:
        print(f"- {rec}")
```

### 5. è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬ (deploy_tools.py)

```python
#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œç»´æŠ¤è„šæœ¬
åŠŸèƒ½ï¼šä¸€é”®å®‰è£…ä¾èµ–ã€åˆå§‹åŒ–ç³»ç»Ÿã€å®šæ—¶ä»»åŠ¡è®¾ç½®
"""

import subprocess
import sys
import os
from pathlib import Path

def install_dependencies():
    """å®‰è£…å¿…è¦çš„Pythonä¾èµ–"""
    requirements = [
        "whoosh>=2.7.4",
        "PyYAML>=6.0",
        "matplotlib>=3.5.0",
        "seaborn>=0.11.0",
        "jinja2>=3.0.0"
    ]
    
    print("æ­£åœ¨å®‰è£…ä¾èµ–åŒ…...")
    for package in requirements:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ“ {package} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError:
            print(f"âœ— {package} å®‰è£…å¤±è´¥")

def initialize_system():
    """åˆå§‹åŒ–ç³»ç»Ÿé…ç½®"""
    print("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    dirs_to_create = ["index", "reports", "logs", "backups"]
    for dir_name in dirs_to_create:
        Path(dir_name).mkdir(exist_ok=True)
        print(f"âœ“ åˆ›å»ºç›®å½•: {dir_name}")
    
    # å¤åˆ¶é…ç½®æ¨¡æ¿
    config_template = """
# çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿé…ç½®æ–‡ä»¶
knowledge_base_path: "."
index_path: "index"
reports_path: "reports"
auto_backup: true
backup_frequency: "daily"
quality_threshold: 80
    """
    
    config_file = Path("config.yaml")
    if not config_file.exists():
        config_file.write_text(config_template.strip())
        print("âœ“ åˆ›å»ºé…ç½®æ–‡ä»¶")
    
    print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

def setup_cron_jobs():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆLinux/Macï¼‰"""
    cron_commands = [
        "# çŸ¥è¯†åº“è‡ªåŠ¨ç»´æŠ¤ä»»åŠ¡",
        "0 2 * * * cd $(pwd) && python3 quality_checker.py >> logs/quality_check.log 2>&1",
        "0 3 * * * cd $(pwd) && python3 index_builder.py --build >> logs/index_build.log 2>&1",
        "0 4 * * 1 cd $(pwd) && python3 dashboard.py >> logs/dashboard.log 2>&1"
    ]
    
    try:
        # è·å–å½“å‰ç”¨æˆ·çš„crontab
        current_crontab = subprocess.check_output(["crontab", "-l"], stderr=subprocess.DECKE).decode()
        
        # æ·»åŠ æ–°ä»»åŠ¡
        new_crontab = current_crontab + "\n" + "\n".join(cron_commands) + "\n"
        
        # æ›´æ–°crontab
        process = subprocess.Popen(["crontab", "-"], stdin=subprocess.PIPE)
        process.communicate(input=new_crontab.encode())
        
        print("âœ“ å®šæ—¶ä»»åŠ¡è®¾ç½®æˆåŠŸ")
        print("å·²æ·»åŠ ä»¥ä¸‹ä»»åŠ¡:")
        for cmd in cron_commands[1:]:
            print(f"  {cmd}")
            
    except Exception as e:
        print(f"âœ— å®šæ—¶ä»»åŠ¡è®¾ç½®å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨æ·»åŠ crontabä»»åŠ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿéƒ¨ç½²å·¥å…·")
    print("=" * 40)
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. å®‰è£…ä¾èµ–åŒ…")
        print("2. åˆå§‹åŒ–ç³»ç»Ÿ")
        print("3. è®¾ç½®å®šæ—¶ä»»åŠ¡")
        print("4. æ‰§è¡Œå®Œæ•´éƒ¨ç½²")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-5): ").strip()
        
        if choice == "1":
            install_dependencies()
        elif choice == "2":
            initialize_system()
        elif choice == "3":
            setup_cron_jobs()
        elif choice == "4":
            print("å¼€å§‹å®Œæ•´éƒ¨ç½²...")
            install_dependencies()
            initialize_system()
            setup_cron_jobs()
            print("âœ… éƒ¨ç½²å®Œæˆï¼")
        elif choice == "5":
            print("å†è§ï¼")
            break
        else:
            print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")

if __name__ == "__main__":
    main()
```

## ğŸ“Š ä½¿ç”¨è¯´æ˜å’Œæœ€ä½³å®è·µ

### å¿«é€Ÿå¼€å§‹

1. **ä¸€é”®éƒ¨ç½²**ï¼š
```bash
python deploy_tools.py
# é€‰æ‹©é€‰é¡¹4æ‰§è¡Œå®Œæ•´éƒ¨ç½²
```

2. **é¦–æ¬¡ä½¿ç”¨**ï¼š
```bash
# æ„å»ºåˆå§‹ç´¢å¼•
python index_builder.py --build

# ç”Ÿæˆè´¨é‡æŠ¥å‘Š
python quality_checker.py

# æŸ¥çœ‹å¥åº·åº¦æŠ¥å‘Š
python dashboard.py
```

3. **æ—¥å¸¸ç»´æŠ¤**ï¼š
```bash
# æœç´¢æ–‡æ¡£
python index_builder.py --search "åˆ›ä¼¤æ²»ç–—"

# æ£€æŸ¥ç‰¹å®šç›®å½•
python quality_checker.py --path ./trauma

# ç”Ÿæˆæœ€æ–°æŠ¥å‘Š
python dashboard.py
```

### è‡ªåŠ¨åŒ–å·¥ä½œæµ

å»ºè®®è®¾ç½®ä»¥ä¸‹è‡ªåŠ¨åŒ–æµç¨‹ï¼š

```bash
# æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œè´¨é‡æ£€æŸ¥
0 2 * * * cd /path/to/knowledge-base && python quality_checker.py

# æ¯æ—¥å‡Œæ™¨3ç‚¹æ›´æ–°ç´¢å¼•
0 3 * * * cd /path/to/knowledge-base && python index_builder.py --build

# æ¯å‘¨ä¸€å‡Œæ™¨4ç‚¹ç”Ÿæˆå¥åº·æŠ¥å‘Š
0 4 * * 1 cd /path/to/knowledge-base && python dashboard.py
```

### ç›‘æ§å’ŒæŠ¥è­¦

```python
# å¯ä»¥æ·»åŠ é‚®ä»¶é€šçŸ¥åŠŸèƒ½
def send_notification(subject, message):
    # é‚®ä»¶å‘é€é€»è¾‘
    pass

# åœ¨è´¨é‡æ£€æŸ¥åæ·»åŠ é€šçŸ¥
if report['overall_score'] < 70:
    send_notification("çŸ¥è¯†åº“å¥åº·åº¦è­¦å‘Š", f"å½“å‰åˆ†æ•°: {report['overall_score']}")
```

è¿™å¥—å·¥å…·ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„çŸ¥è¯†åº“ç®¡ç†å’Œè´¨é‡æ§åˆ¶è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–åœ°ç»´æŠ¤çŸ¥è¯†åº“çš„å¥åº·çŠ¶æ€ï¼Œç¡®ä¿å†…å®¹è´¨é‡å’Œç»“æ„å®Œæ•´æ€§ã€‚