#!/usr/bin/env python3
"""
æœ¯è¯­åˆ†æå·¥å…· - ç³»ç»Ÿæ€§åˆ†æçŸ¥è¯†åº“æ–‡æ¡£ä¸­çš„ä¸“ä¸šæœ¯è¯­
"""

import os
import re
import json
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple

class TerminologyAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.terms_found = defaultdict(list)  # æœ¯è¯­ -> [(æ–‡ä»¶, è¡Œå·, ä¸Šä¸‹æ–‡)]
        self.term_categories = {}
        self.stop_words = self._load_stop_words()
        
    def _load_stop_words(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯åˆ—è¡¨"""
        return {
            'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'ç€', 'è¿‡',
            'this', 'that', 'these', 'those', 'which', 'what', 'how', 'why', 'when',
            'where', 'who', 'whom', 'whose', 'will', 'would', 'could', 'should',
            'can', 'may', 'might', 'must', 'shall', 'about', 'above', 'across',
            'after', 'against', 'along', 'among', 'around', 'before', 'behind',
            'below', 'beneath', 'beside', 'between', 'beyond', 'during', 'except',
            'for', 'from', 'into', 'near', 'of', 'off', 'on', 'out', 'over',
            'since', 'through', 'throughout', 'till', 'to', 'toward', 'under',
            'until', 'upon', 'with', 'within', 'without'
        }
    
    def find_markdown_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶ï¼ˆæ’é™¤ç³»ç»Ÿç›®å½•ï¼‰"""
        md_files = []
        exclude_dirs = {'.git', '.trae', 'tools'}
        
        # ä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹æœç´¢
        root_path = self.base_path.parent
        
        for file_path in root_path.rglob("*.md"):
            if not any(exclude_dir in str(file_path) for exclude_dir in exclude_dirs):
                md_files.append(file_path)
                
        return sorted(md_files)
    
    def extract_potential_terms(self, text: str, line_num: int) -> List[Tuple[str, int, str]]:
        """ä»æ–‡æœ¬ä¸­æå–æ½œåœ¨æœ¯è¯­"""
        terms = []
        
        # åŒ¹é…å¯èƒ½çš„æœ¯è¯­æ¨¡å¼
        patterns = [
            r'\*\*(.*?)\*\*',  # åŠ ç²—æ–‡æœ¬
            r'`(.*?)`',        # ä»£ç /æœ¯è¯­æ ‡è®°
            r'_([^_]+)_',      # æ–œä½“æ–‡æœ¬
            r'"([^"]+)"',      # å¼•å·åŒ…å›´çš„æœ¯è¯­
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)',  # é©¼å³°å¼çŸ­è¯­
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                term = match.group(1).strip()
                if self._is_valid_term(term):
                    context = self._get_context(text, match.start(), match.end())
                    terms.append((term, line_num, context))
                    
        return terms
    
    def _is_valid_term(self, term: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæœ¯è¯­"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(term) < 2 or len(term) > 50:
            return False
            
        # æ’é™¤çº¯æ•°å­—
        if term.isdigit():
            return False
            
        # æ’é™¤åœç”¨è¯
        if term.lower() in self.stop_words:
            return False
            
        # æ’é™¤å¸¸è§éæœ¯è¯­è¯æ±‡
        common_words = {'æˆ‘ä»¬', 'ä»–ä»¬', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'å¦‚æœ', 'è™½ç„¶'}
        if term in common_words:
            return False
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­—æ¯ï¼ˆè‡³å°‘è¦æœ‰è‹±æ–‡æˆ–ä¸­æ–‡å­—ç¬¦ï¼‰
        if not re.search(r'[a-zA-Z\u4e00-\u9fff]', term):
            return False
            
        return True
    
    def _get_context(self, text: str, start: int, end: int, context_length: int = 50) -> str:
        """è·å–æœ¯è¯­ä¸Šä¸‹æ–‡"""
        left_start = max(0, start - context_length)
        right_end = min(len(text), end + context_length)
        
        left_context = text[left_start:start].strip()
        right_context = text[end:right_end].strip()
        
        return f"...{left_context}[{text[start:end]}]{right_context}..."
    
    def categorize_term(self, term: str, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œæœ¯è¯­å†…å®¹è¿›è¡Œåˆ†ç±»"""
        term_lower = term.lower()
        file_path_lower = file_path.lower()
        
        # æ ¹æ®æ–‡ä»¶è·¯å¾„åˆ†ç±»
        if any(keyword in file_path_lower for keyword in ['psychology', 'å¿ƒç†', 'cbt', 'dbt']):
            return 'å¿ƒç†å­¦æ ¸å¿ƒæœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['buddhism', 'ä½›æ•™', 'zen', 'ç¦…', 'mindfulness']):
            return 'ä¸œæ–¹ä¼ ç»Ÿæ™ºæ…§æœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['therapy', 'æ²»ç–—', 'intervention']):
            return 'æ²»ç–—æ–¹æ³•ä¸æŠ€æœ¯æœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['brain', 'neuro', 'ç¥ç»', 'cortisol', 'hpa']):
            return 'ç¥ç»ç§‘å­¦ä¸ç”Ÿç‰©åŒ»å­¦æœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['east-asian', 'china', 'japan', 'syncretism']):
            return 'è·¨æ–‡åŒ–ä¸æ•´åˆæœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['music', 'art', 'sensory', 'sound']):
            return 'è‰ºæœ¯ä¸æ„Ÿå®˜ç–—æ„ˆæœ¯è¯­'
        elif any(keyword in file_path_lower for keyword in ['assessment', 'measure', 'æµ‹è¯„']):
            return 'è¯„ä¼°ä¸æµ‹é‡æœ¯è¯­'
        else:
            return 'é€šç”¨æœ¯è¯­'
    
    def analyze_document(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡æ¡£ä¸­çš„æœ¯è¯­"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            doc_terms = []
            relative_path = str(file_path.relative_to(self.base_path))
            
            for line_num, line in enumerate(lines, 1):
                # è·³è¿‡ä»£ç å—ã€è¡¨æ ¼å’Œæ ‡é¢˜è¡Œ
                if (line.strip().startswith('```') or 
                    line.strip().startswith('|') or
                    line.strip().startswith('#') or
                    len(line.strip()) < 3):
                    continue
                    
                terms = self.extract_potential_terms(line, line_num)
                for term, line_num, context in terms:
                    category = self.categorize_term(term, relative_path)
                    doc_terms.append({
                        'term': term,
                        'category': category,
                        'line': line_num,
                        'context': context,
                        'file': relative_path
                    })
                    
            return {
                'file': relative_path,
                'total_terms': len(doc_terms),
                'unique_terms': len(set(t['term'].lower() for t in doc_terms)),
                'terms': doc_terms
            }
            
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return {'file': str(file_path), 'error': str(e)}
    
    def run_full_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æœ¯è¯­åˆ†æ"""
        print("ğŸ” å¼€å§‹æœ¯è¯­åˆ†æ...")
        
        md_files = self.find_markdown_files()
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ªå¾…åˆ†æçš„Markdownæ–‡ä»¶")
        
        analysis_results = {
            'total_files': len(md_files),
            'analyzed_files': [],
            'all_terms': defaultdict(list),
            'category_stats': defaultdict(int),
            'term_frequency': Counter()
        }
        
        for i, file_path in enumerate(md_files, 1):
            print(f"æ­£åœ¨åˆ†æ ({i}/{len(md_files)}): {file_path.name}")
            
            result = self.analyze_document(file_path)
            analysis_results['analyzed_files'].append(result)
            
            # æ”¶é›†æœ¯è¯­æ•°æ®
            if 'terms' in result:
                for term_data in result['terms']:
                    term_key = term_data['term'].lower()
                    analysis_results['all_terms'][term_key].append(term_data)
                    analysis_results['category_stats'][term_data['category']] += 1
                    analysis_results['term_frequency'][term_data['term']] += 1
        
        print(f"\nğŸ“Š åˆ†æå®Œæˆ!")
        print(f"æ€»æ–‡ä»¶æ•°: {analysis_results['total_files']}")
        print(f"å‘ç°æœ¯è¯­ç§ç±»: {len(analysis_results['all_terms'])}")
        print(f"æœ¯è¯­æ€»å‡ºç°æ¬¡æ•°: {sum(analysis_results['term_frequency'].values())}")
        
        return analysis_results
    
    def generate_analysis_report(self, analysis_results: Dict, output_file: str = "TERMINOLOGY_ANALYSIS_REPORT.md"):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_content = "# æœ¯è¯­åˆ†ææŠ¥å‘Š\n\n"
        report_content += f"åˆ†ææ—¶é—´: {self.get_current_time()}\n\n"
        
        # æ€»ä½“ç»Ÿè®¡
        report_content += "## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n"
        report_content += f"- åˆ†ææ–‡ä»¶æ€»æ•°: {analysis_results['total_files']}\n"
        report_content += f"- å‘ç°æœ¯è¯­ç§ç±»: {len(analysis_results['all_terms'])}\n"
        report_content += f"- æœ¯è¯­æ€»å‡ºç°æ¬¡æ•°: {sum(analysis_results['term_frequency'].values())}\n"
        report_content += f"- å¹³å‡æ¯æ–‡ä»¶æœ¯è¯­æ•°: {sum(len(f.get('terms', [])) for f in analysis_results['analyzed_files']) / len(analysis_results['analyzed_files']):.1f}\n\n"
        
        # ç±»åˆ«ç»Ÿè®¡
        report_content += "## ğŸ“š æœ¯è¯­ç±»åˆ«åˆ†å¸ƒ\n\n"
        report_content += "| ç±»åˆ« | æœ¯è¯­æ•°é‡ | å æ¯” |\n"
        report_content += "|------|----------|------|\n"
        
        total_terms = sum(analysis_results['category_stats'].values())
        for category, count in sorted(analysis_results['category_stats'].items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_terms * 100) if total_terms > 0 else 0
            report_content += f"| {category} | {count} | {percentage:.1f}% |\n"
        
        report_content += "\n"
        
        # é«˜é¢‘æœ¯è¯­
        report_content += "## ğŸ” é«˜é¢‘æœ¯è¯­ Top 50\n\n"
        report_content += "| æ’å | æœ¯è¯­ | å‡ºç°æ¬¡æ•° | é¦–æ¬¡å‡ºç°æ–‡ä»¶ |\n"
        report_content += "|------|------|----------|----------------|\n"
        
        top_terms = analysis_results['term_frequency'].most_common(50)
        for rank, (term, count) in enumerate(top_terms, 1):
            first_occurrence = analysis_results['all_terms'][term.lower()][0]['file']
            report_content += f"| {rank} | {term} | {count} | {first_occurrence} |\n"
        
        report_content += "\n"
        
        # æ¯ä¸ªæ–‡ä»¶çš„æœ¯è¯­ç»Ÿè®¡
        report_content += "## ğŸ“„ æ–‡ä»¶æœ¯è¯­ç»Ÿè®¡\n\n"
        report_content += "| æ–‡ä»¶ | æœ¯è¯­æ•°é‡ | å”¯ä¸€æœ¯è¯­ | ä¸»è¦ç±»åˆ« |\n"
        report_content += "|------|----------|----------|----------|\n"
        
        for file_result in analysis_results['analyzed_files']:
            if 'terms' in file_result:
                categories = Counter(t['category'] for t in file_result['terms'])
                main_category = categories.most_common(1)[0][0] if categories else 'N/A'
                report_content += f"| {file_result['file']} | {file_result['total_terms']} | {file_result['unique_terms']} | {main_category} |\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.base_path / output_file
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report_path
    
    def export_terms_for_dictionary(self, analysis_results: Dict, output_file: str = "extracted_terms.json"):
        """å¯¼å‡ºæœ¯è¯­ç”¨äºè¯å…¸æ›´æ–°"""
        terms_data = {
            'extraction_time': self.get_current_time(),
            'total_unique_terms': len(analysis_results['all_terms']),
            'categories': {},
            'terms': {}
        }
        
        # æŒ‰ç±»åˆ«ç»„ç»‡æœ¯è¯­
        for category in analysis_results['category_stats'].keys():
            terms_data['categories'][category] = {
                'term_count': analysis_results['category_stats'][category],
                'terms': []
            }
        
        # æ·»åŠ å…·ä½“æœ¯è¯­
        for term_lower, occurrences in analysis_results['all_terms'].items():
            # è·å–è¯¥æœ¯è¯­çš„æ‰€æœ‰å˜ä½“å½¢å¼
            term_variants = list(set(occ['term'] for occ in occurrences))
            primary_term = max(term_variants, key=len)  # é€‰æ‹©æœ€é•¿çš„ä½œä¸ºä¸»æœ¯è¯­
            
            # ç¡®å®šç±»åˆ«ï¼ˆå¤šæ•°æŠ•ç¥¨ï¼‰
            categories = [occ['category'] for occ in occurrences]
            main_category = Counter(categories).most_common(1)[0][0]
            
            # è·å–é¦–æ¬¡å‡ºç°çš„ä¸Šä¸‹æ–‡ä½œä¸ºå®šä¹‰å‚è€ƒ
            first_occurrence = occurrences[0]
            
            terms_data['terms'][primary_term] = {
                'category': main_category,
                'variants': term_variants,
                'frequency': len(occurrences),
                'first_context': first_occurrence['context'],
                'files_mentioned': list(set(occ['file'] for occ in occurrences)),
                'sample_definition': ''  # éœ€è¦äººå·¥å®Œå–„
            }
            
            terms_data['categories'][main_category]['terms'].append(primary_term)
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_path = self.base_path / output_file
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(terms_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ æœ¯è¯­æ•°æ®å·²å¯¼å‡ºåˆ°: {json_path}")
        return json_path
    
    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = TerminologyAnalyzer()
    results = analyzer.run_full_analysis()
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_analysis_report(results)
    
    # å¯¼å‡ºæœ¯è¯­æ•°æ®
    analyzer.export_terms_for_dictionary(results)
    
    print("\nâœ… æœ¯è¯­åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()