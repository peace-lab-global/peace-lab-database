#!/usr/bin/env python3
"""
æœ¯è¯­è¯å…¸æ›´æ–°å·¥å…· - å°†åˆ†ææå–çš„æœ¯è¯­æ•´ç†ä¸ºæ ‡å‡†è¯å…¸æ ¼å¼
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Set

class DictionaryUpdater:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.existing_terms = {}  # ç°æœ‰æœ¯è¯­è¯å…¸
        self.new_terms = {}       # æ–°æå–çš„æœ¯è¯­
        self.updated_terms = {}   # æ›´æ–°åçš„æœ¯è¯­
        
    def load_existing_dictionary(self, dict_path: str = "resources/Terminology_Dictionary.md"):
        """åŠ è½½ç°æœ‰çš„æœ¯è¯­è¯å…¸"""
        dict_file = self.base_path / dict_path
        if dict_file.exists():
            with open(dict_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # è§£æç°æœ‰æœ¯è¯­ï¼ˆç®€å•æå–è¡¨æ ¼å†…å®¹ï¼‰
                self.parse_existing_terms(content)
    
    def parse_existing_terms(self, content: str):
        """è§£æç°æœ‰æœ¯è¯­è¯å…¸å†…å®¹"""
        # æå–è¡¨æ ¼ä¸­çš„æœ¯è¯­
        lines = content.split('\n')
        in_table = False
        
        for line in lines:
            if '|' in line and 'ä¸­æ–‡æœ¯è¯­' in line:
                in_table = True
                continue
            elif in_table and line.strip() == '':
                in_table = False
                continue
                
            if in_table and '|' in line and not any(header in line for header in 
                ['----', 'ä¸­æ–‡æœ¯è¯­', 'è‹±æ–‡æ ‡å‡†æœ¯è¯­', 'å®šä¹‰', 'ä½¿ç”¨åœºæ™¯']):
                # è§£æè¡¨æ ¼è¡Œ
                parts = [p.strip() for p in line.split('|')[1:-1]]
                if len(parts) >= 2:
                    chinese_term = parts[0]
                    english_term = parts[1]
                    self.existing_terms[chinese_term.lower()] = {
                        'chinese': chinese_term,
                        'english': english_term,
                        'definition': parts[2] if len(parts) > 2 else '',
                        'usage': parts[3] if len(parts) > 3 else ''
                    }
    
    def load_extracted_terms(self, extracted_path: str = "extracted_terms.json"):
        """åŠ è½½æå–çš„æœ¯è¯­æ•°æ®"""
        extracted_file = self.base_path / extracted_path
        if extracted_file.exists():
            with open(extracted_file, 'r', encoding='utf-8') as f:
                self.new_terms = json.load(f)
    
    def categorize_professional_terms(self) -> Dict[str, List[Dict]]:
        """åˆ†ç±»ä¸“ä¸šæœ¯è¯­"""
        categorized = {
            'å¿ƒç†å­¦æ ¸å¿ƒæœ¯è¯­': [],
            'ä¸œæ–¹ä¼ ç»Ÿæ™ºæ…§æœ¯è¯­': [],
            'æ²»ç–—æ–¹æ³•ä¸æŠ€æœ¯æœ¯è¯­': [],
            'ç¥ç»ç§‘å­¦ä¸ç”Ÿç‰©åŒ»å­¦æœ¯è¯­': [],
            'è·¨æ–‡åŒ–ä¸æ•´åˆæœ¯è¯­': [],
            'è‰ºæœ¯ä¸æ„Ÿå®˜ç–—æ„ˆæœ¯è¯­': [],
            'è¯„ä¼°ä¸æµ‹é‡æœ¯è¯­': []
        }
        
        # ä»æå–çš„æ•°æ®ä¸­ç­›é€‰ä¸“ä¸šæœ¯è¯­
        if 'terms' in self.new_terms:
            for term, data in self.new_terms['terms'].items():
                if self.is_professional_term(term, data):
                    category = self.determine_term_category(term, data)
                    categorized[category].append({
                        'chinese': term,
                        'english': self.extract_english_term(term, data),
                        'definition': '',  # éœ€è¦äººå·¥å®Œå–„
                        'usage': '',       # éœ€è¦äººå·¥å®Œå–„
                        'frequency': data.get('frequency', 0),
                        'files': data.get('files_mentioned', [])
                    })
        
        return categorized
    
    def is_professional_term(self, term: str, data: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸“ä¸šæœ¯è¯­"""
        # æ’é™¤éä¸“ä¸šæœ¯è¯­
        non_professional_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[a-z]+$',  # çº¯å°å†™è‹±æ–‡å•è¯
            r'^(æ˜¯|çš„|åœ¨|æœ‰|å’Œ|ä¸|æˆ–|ä½†|è€Œ|äº†|ç€|è¿‡)$',  # ä¸­æ–‡è™šè¯
            r'^(Peace Lab|Allen Galler|Project|Database)$',  # é¡¹ç›®åç§°
            r'.*\.(md|py|json)$',  # æ–‡ä»¶æ‰©å±•å
            r'^No\d+$',  # ç¼–å·
            r'^(Brandenburg|Bach|Beethoven|Chopin|Mozart)$'  # ä½œæ›²å®¶å§“å
        ]
        
        for pattern in non_professional_patterns:
            if re.match(pattern, term, re.IGNORECASE):
                return False
        
        # åŒ…å«ä¸“ä¸šç‰¹å¾çš„æœ¯è¯­
        professional_indicators = [
            'ç–—æ³•', 'æ²»ç–—', 'å¿ƒç†å­¦', 'ç¥ç»', 'è®¤çŸ¥', 'è¡Œä¸º', 'æ­£å¿µ', 'å†¥æƒ³',
            'ä½›æ•™', 'é“æ•™', 'ç¦…å®—', 'å†…è§‚', 'æ…ˆæ‚²', 'ä¾æ‹', 'åˆ›ä¼¤', 'PTSD',
            'CBT', 'DBT', 'EMDR', 'MDMA', 'ç¥ç»å¯å¡‘æ€§', 'é»˜è®¤æ¨¡å¼ç½‘ç»œ',
            'HPAè½´', 'çš®è´¨é†‡', 'å¤šè¿·èµ°', 'éŸ³ä¹ç–—æ³•', 'èŠ³é¦™ç–—æ³•'
        ]
        
        return any(indicator in term for indicator in professional_indicators)
    
    def determine_term_category(self, term: str, data: Dict) -> str:
        """ç¡®å®šæœ¯è¯­ç±»åˆ«"""
        term_lower = term.lower()
        
        # æ ¹æ®æœ¯è¯­å†…å®¹åˆ†ç±»
        if any(word in term_lower for word in ['å¿ƒç†å­¦', 'è®¤çŸ¥', 'è¡Œä¸º', 'cbt', 'dbt', 'ä¾æ‹', 'å‘å±•']):
            return 'å¿ƒç†å­¦æ ¸å¿ƒæœ¯è¯­'
        elif any(word in term_lower for word in ['ä½›æ•™', 'é“æ•™', 'ç¦…', 'æ­£å¿µ', 'å†…è§‚', 'æ…ˆæ‚²', 'æ¶…æ§ƒ']):
            return 'ä¸œæ–¹ä¼ ç»Ÿæ™ºæ…§æœ¯è¯­'
        elif any(word in term_lower for word in ['ç–—æ³•', 'æ²»ç–—', 'å¹²é¢„', 'emdr', 'mdma']):
            return 'æ²»ç–—æ–¹æ³•ä¸æŠ€æœ¯æœ¯è¯­'
        elif any(word in term_lower for word in ['ç¥ç»', 'å¤§è„‘', 'çš®è´¨é†‡', 'hpa', 'å¤šè¿·èµ°']):
            return 'ç¥ç»ç§‘å­¦ä¸ç”Ÿç‰©åŒ»å­¦æœ¯è¯­'
        elif any(word in term_lower for word in ['è·¨æ–‡åŒ–', 'æ•´åˆ', 'syncretism', 'ä¸‰æ•™']):
            return 'è·¨æ–‡åŒ–ä¸æ•´åˆæœ¯è¯­'
        elif any(word in term_lower for word in ['éŸ³ä¹', 'è‰ºæœ¯', 'æ„Ÿå®˜', 'èŠ³é¦™', 'å£°éŸ³']):
            return 'è‰ºæœ¯ä¸æ„Ÿå®˜ç–—æ„ˆæœ¯è¯­'
        elif any(word in term_lower for word in ['æµ‹è¯„', 'æµ‹é‡', 'é‡è¡¨', 'inventory']):
            return 'è¯„ä¼°ä¸æµ‹é‡æœ¯è¯­'
        else:
            return 'å¿ƒç†å­¦æ ¸å¿ƒæœ¯è¯­'  # é»˜è®¤å½’ç±»
    
    def extract_english_term(self, chinese_term: str, data: Dict) -> str:
        """ä»ä¸­æ–‡æœ¯è¯­ä¸­æå–æˆ–æ¨æ–­è‹±æ–‡æœ¯è¯­"""
        # ç›´æ¥åŒ¹é…å·²çŸ¥æœ¯è¯­
        known_translations = {
            'è®¤çŸ¥è¡Œä¸ºç–—æ³•': 'Cognitive Behavioral Therapy (CBT)',
            'è¾©è¯è¡Œä¸ºç–—æ³•': 'Dialectical Behavior Therapy (DBT)',
            'æ­£å¿µ': 'Mindfulness',
            'æ…ˆæ‚²': 'Compassion',
            'åˆ›ä¼¤ååº”æ¿€éšœç¢': 'Post-Traumatic Stress Disorder (PTSD)',
            'ç¥ç»å¯å¡‘æ€§': 'Neuroplasticity',
            'é»˜è®¤æ¨¡å¼ç½‘ç»œ': 'Default Mode Network (DMN)',
            'HPAè½´': 'HPA Axis',
            'çš®è´¨é†‡': 'Cortisol',
            'éŸ³ä¹ç–—æ³•': 'Music Therapy',
            'èŠ³é¦™ç–—æ³•': 'Aromatherapy'
        }
        
        if chinese_term in known_translations:
            return known_translations[chinese_term]
        
        # ä»ä¸Šä¸‹æ–‡ä¸­æå–è‹±æ–‡æœ¯è¯­
        context = data.get('first_context', '')
        english_matches = re.findall(r'\b[A-Z][a-zA-Z]*(?:\s+[A-Z][a-zA-Z]*)*\b', context)
        if english_matches:
            return english_matches[0]
        
        return ''  # æœªçŸ¥ç¿»è¯‘
    
    def generate_updated_dictionary(self) -> str:
        """ç”Ÿæˆæ›´æ–°åçš„æœ¯è¯­è¯å…¸"""
        categorized_terms = self.categorize_professional_terms()
        
        # ç”Ÿæˆè¯å…¸å†…å®¹
        content = "# ä¸“ä¸šæœ¯è¯­è¯å…¸ (Professional Terminology Dictionary)\n\n"
        content += "## ğŸ“‹ æœ¯è¯­æ ‡å‡†åŒ–è§„èŒƒ\n\n"
        content += "æœ¬è¯å…¸æ—¨åœ¨ä¸ºå¹³é™å®éªŒå®¤çŸ¥è¯†åº“å»ºç«‹ç»Ÿä¸€çš„ä¸“ä¸šæœ¯è¯­æ ‡å‡†ï¼Œç¡®ä¿æœ¯è¯­ä½¿ç”¨çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚\n\n"
        content += "---\n\n"
        
        # æŒ‰ç±»åˆ«ç”Ÿæˆè¡¨æ ¼
        for category, terms in categorized_terms.items():
            if terms:  # åªæ˜¾ç¤ºæœ‰æœ¯è¯­çš„ç±»åˆ«
                content += f"## ğŸ§  {category}\n\n"
                content += "| ä¸­æ–‡æœ¯è¯­ | è‹±æ–‡æ ‡å‡†æœ¯è¯­ | å®šä¹‰ | ä½¿ç”¨åœºæ™¯ | ç›¸å…³æ–‡æ¡£ |\n"
                content += "|---------|-------------|------|----------|----------|\n"
                
                # æŒ‰é¢‘ç‡æ’åºï¼Œå±•ç¤ºé«˜é¢‘æœ¯è¯­
                sorted_terms = sorted(terms, key=lambda x: x['frequency'], reverse=True)[:30]
                
                for term_data in sorted_terms:
                    chinese = term_data['chinese']
                    english = term_data['english'] or 'å¾…ç¡®å®š'
                    definition = term_data['definition'] or 'å¾…å®Œå–„'
                    usage = term_data['usage'] or 'å¾…ç¡®å®š'
                    files = ', '.join(term_data['files'][:2]) + ('...' if len(term_data['files']) > 2 else '')
                    
                    content += f"| {chinese} | {english} | {definition} | {usage} | {files} |\n"
                
                content += "\n---\n\n"
        
        content += "*æœ€åæ›´æ–°ï¼š" + self.get_current_time() + "*  \n"
        content += "*ç»´æŠ¤è€…ï¼šå¹³é™å®éªŒå®¤æœ¯è¯­å§”å‘˜ä¼š*"
        
        return content
    
    def save_updated_dictionary(self, output_path: str = "resources/Updated_Terminology_Dictionary.md"):
        """ä¿å­˜æ›´æ–°åçš„æœ¯è¯­è¯å…¸"""
        content = self.generate_updated_dictionary()
        output_file = self.base_path / output_path
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“„ æ›´æ–°åçš„æœ¯è¯­è¯å…¸å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    def run_update(self):
        """æ‰§è¡Œå®Œæ•´çš„è¯å…¸æ›´æ–°æµç¨‹"""
        print("ğŸ”„ å¼€å§‹æœ¯è¯­è¯å…¸æ›´æ–°...")
        
        # åŠ è½½ç°æœ‰è¯å…¸å’Œæ–°æœ¯è¯­
        self.load_existing_dictionary()
        self.load_extracted_terms()
        
        print(f"ğŸ“Š ç°æœ‰æœ¯è¯­æ•°é‡: {len(self.existing_terms)}")
        print(f"ğŸ“Š æ–°æå–æœ¯è¯­æ•°é‡: {len(self.new_terms.get('terms', {}))}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜æ›´æ–°åçš„è¯å…¸
        output_file = self.save_updated_dictionary()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        categorized = self.categorize_professional_terms()
        total_new_terms = sum(len(terms) for terms in categorized.values())
        
        print(f"\nâœ… æœ¯è¯­è¯å…¸æ›´æ–°å®Œæˆ!")
        print(f"ğŸ“ æ€»ä¸“ä¸šæœ¯è¯­æ•°: {total_new_terms}")
        for category, terms in categorized.items():
            if terms:
                print(f"   {category}: {len(terms)} ä¸ªæœ¯è¯­")
        
        return output_file

def main():
    """ä¸»å‡½æ•°"""
    updater = DictionaryUpdater()
    updater.run_update()

if __name__ == "__main__":
    main()