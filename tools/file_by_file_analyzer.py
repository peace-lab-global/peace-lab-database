#!/usr/bin/env python3
"""
é€æ–‡ä»¶æœ¯è¯­åˆ†æå·¥å…· - ç²¾ç¡®åˆ†ææ¯ä¸ªæ–‡æ¡£ä¸­çš„ä¸“ä¸šæœ¯è¯­é—æ¼
"""

import os
import re
import json
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple

class FileByFileAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.known_terms = self._load_known_terms()
        self.missing_terms = defaultdict(list)
        self.file_analysis = {}
        
    def _load_known_terms(self) -> Set[str]:
        """åŠ è½½å·²çŸ¥æœ¯è¯­åº“"""
        known_terms = set()
        
        # ä»ç°æœ‰æœ¯è¯­è¯å…¸ä¸­æå–æœ¯è¯­
        dict_path = self.base_path / "resources" / "Terminology_Dictionary.md"
        if dict_path.exists():
            with open(dict_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–è¡¨æ ¼ä¸­çš„æœ¯è¯­
                lines = content.split('\n')
                for line in lines:
                    if '|' in line and not any(header in line for header in 
                        ['----', 'ä¸­æ–‡æœ¯è¯­', 'è‹±æ–‡æ ‡å‡†æœ¯è¯­', 'å®šä¹‰']):
                        parts = [p.strip() for p in line.split('|')[1:-1]]
                        if len(parts) >= 1 and parts[0]:
                            known_terms.add(parts[0].strip('*'))
                            
        return known_terms
    
    def get_all_markdown_files(self) -> List[Path]:
        """è·å–æ‰€æœ‰Markdownæ–‡ä»¶ï¼ˆæ’é™¤ç³»ç»Ÿç›®å½•ï¼‰"""
        md_files = []
        exclude_dirs = {'.git', '.trae', 'tools', 'template'}
        
        for file_path in self.base_path.rglob("*.md"):
            if not any(exclude_dir in str(file_path) for exclude_dir in exclude_dirs):
                md_files.append(file_path)
                
        return sorted(md_files)
    
    def extract_domain_specific_terms(self, content: str, domain: str) -> List[str]:
        """æ ¹æ®é¢†åŸŸæå–ä¸“ä¸šæœ¯è¯­"""
        terms = []
        
        domain_patterns = {
            'psychology': [
                r'è®¤çŸ¥è¡Œä¸ºç–—æ³•|CBT|è¾©è¯è¡Œä¸ºç–—æ³•|DBT|æ­£å¿µ|å†¥æƒ³|ç„¦è™‘|æŠ‘éƒ|åˆ›ä¼¤',
                r'ä¾æ‹|å‘å±•|äººæ ¼|æƒ…ç»ª|è¡Œä¸º|è®¤çŸ¥',
                r'PTSD|EMDR|ACT|MBSR|CBT|DBT'
            ],
            'eastern_wisdom': [
                r'ä½›æ•™|ç¦…å®—|é“å®¶|é“æ•™|æ¶…æ§ƒ|ç¼˜èµ·|ç©ºæ€§|æ— å¸¸|æ— æˆ‘',
                r'é“|å¾·|æ— ä¸º|é˜´é˜³|æ°”|å†…ä¸¹|æ­£å¿µ|æ…ˆæ‚²',
                r'Buddha|Dharma|Sangha|Nirvana|Karma|Zen|Tao'
            ],
            'therapy': [
                r'æ²»ç–—|ç–—æ³•|å¹²é¢„|å’¨è¯¢|è¾…å¯¼|å¿ƒç†|ç²¾ç¥',
                r'EMDR|MDMA|æš´éœ²|æ­£å¿µ|è®¤çŸ¥|è¡Œä¸º|å®¶åº­|å›¢ä½“',
                r'æ²»ç–—å¸ˆ|å’¨è¯¢å¸ˆ|å¿ƒç†å¸ˆ|åŒ»å¸ˆ'
            ],
            'neuroscience': [
                r'ç¥ç»|å¤§è„‘|è„‘åŒº|ç¥ç»é€’è´¨|æ¿€ç´ |çš®è´¨é†‡|HPA|DMN',
                r'ç¥ç»å¯å¡‘æ€§|é»˜è®¤æ¨¡å¼|æä»æ ¸|å‰é¢å¶|æµ·é©¬',
                r'Neuro|Brain|Cortex|Hormone|Neurotransmitter'
            ],
            'assessment': [
                r'é‡è¡¨|æµ‹è¯„|è¯„ä¼°|æµ‹é‡|è¯Šæ–­|ç­›æŸ¥',
                r'GAD|BAI|HAM|PCL|IES|STAI',
                r'ä¿¡åº¦|æ•ˆåº¦|å¸¸æ¨¡|æ ‡å‡†åŒ–'
            ]
        }
        
        if domain in domain_patterns:
            for pattern in domain_patterns[domain]:
                matches = re.findall(pattern, content, re.IGNORECASE)
                terms.extend(matches)
                
        return list(set(terms))
    
    def determine_file_domain(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šé¢†åŸŸ"""
        path_str = str(file_path).lower()
        
        if any(keyword in path_str for keyword in ['psychology', 'å¿ƒç†', 'cbt', 'dbt', 'anxiety', 'depression']):
            return 'psychology'
        elif any(keyword in path_str for keyword in ['buddhism', 'ä½›æ•™', 'zen', 'ç¦…', 'dao', 'é“', 'mindfulness']):
            return 'eastern_wisdom'
        elif any(keyword in path_str for keyword in ['therapy', 'æ²»ç–—', 'intervention', 'treatment']):
            return 'therapy'
        elif any(keyword in path_str for keyword in ['brain', 'neuro', 'ç¥ç»', 'cortisol', 'hpa', 'dmn']):
            return 'neuroscience'
        elif any(keyword in path_str for keyword in ['assessment', 'measure', 'æµ‹è¯„', 'é‡è¡¨']):
            return 'assessment'
        else:
            return 'general'
    
    def analyze_single_file(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            relative_path = str(file_path.relative_to(self.base_path))
            domain = self.determine_file_domain(file_path)
            
            # æå–é¢†åŸŸç‰¹å®šæœ¯è¯­
            domain_terms = self.extract_domain_specific_terms(content, domain)
            
            # è¯†åˆ«ç¼ºå¤±æœ¯è¯­
            missing_from_dict = []
            for term in domain_terms:
                if term not in self.known_terms:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„ä¸“ä¸šæœ¯è¯­
                    if self._is_professional_term(term):
                        missing_from_dict.append(term)
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'file': relative_path,
                'domain': domain,
                'total_domain_terms': len(domain_terms),
                'missing_terms': missing_from_dict,
                'missing_count': len(missing_from_dict),
                'content_length': len(content),
                'word_count': len(content.split())
            }
            
            # è®°å½•ç¼ºå¤±æœ¯è¯­
            for term in missing_from_dict:
                self.missing_terms[term].append({
                    'file': relative_path,
                    'domain': domain,
                    'context': content[:200] + '...' if len(content) > 200 else content
                })
            
            return stats
            
        except Exception as e:
            return {
                'file': str(file_path),
                'error': str(e),
                'domain': 'error'
            }
    
    def _is_professional_term(self, term: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸“ä¸šæœ¯è¯­"""
        # æ’é™¤éä¸“ä¸šæœ¯è¯­
        non_professional = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[a-z]{1,3}$',  # çŸ­è‹±æ–‡å•è¯
            r'^(æ˜¯|çš„|åœ¨|æœ‰|å’Œ|ä¸|æˆ–|ä½†|è€Œ|äº†|ç€|è¿‡|æ²»ç–—|è®¤çŸ¥|è¡Œä¸º|ç–—æ³•|æ²»ç–—å¸ˆ)$',  # åŸºç¡€è¯æ±‡
            r'.*\.(md|py|json)$',  # æ–‡ä»¶æ‰©å±•å
            r'^No\d+$',  # ç¼–å·
            r'^(Brandenburg|Bach|Beethoven|Chopin|Mozart)$'  # ä½œæ›²å®¶å§“å
        ]
        
        for pattern in non_professional:
            if re.match(pattern, term, re.IGNORECASE):
                return False
        
        # ä¸“ä¸šæœ¯è¯­ç‰¹å¾ - æ›´ä¸¥æ ¼çš„ç­›é€‰
        professional_indicators = [
            'ç¥ç»å¯å¡‘æ€§', 'é»˜è®¤æ¨¡å¼ç½‘ç»œ', 'HPAè½´', 'çš®è´¨é†‡', 'å¤šè¿·èµ°ç¥ç»ç†è®º',
            'çœ¼åŠ¨è„±æ•å†åŠ å·¥', 'è¾©è¯è¡Œä¸ºç–—æ³•', 'æ¥å—æ‰¿è¯ºç–—æ³•', 'æ­£å¿µè®¤çŸ¥ç–—æ³•',
            'åˆ›ä¼¤ååº”æ¿€éšœç¢', 'å¹¿æ³›æ€§ç„¦è™‘éšœç¢', 'å¼ºè¿«ç—‡', 'è¾¹ç¼˜æ€§äººæ ¼éšœç¢',
            'ä½›æ•™å¿ƒç†å­¦', 'ç¦…å®—ç¥ç»ç§‘å­¦', 'é“å®¶å…»ç”Ÿ', 'å†…ä¸¹ä¿®æŒ',
            'éŸ³ä¹æ²»ç–—', 'è‰ºæœ¯æ²»ç–—', 'èˆè¹ˆæ²»ç–—', 'èŠ³é¦™ç–—æ³•',
            'ç”Ÿç‰©åé¦ˆ', 'ç¥ç»è°ƒæ§', 'EEGç”Ÿç‰©åé¦ˆ', 'MDMAè¾…åŠ©æ²»ç–—'
        ]
        
        return any(indicator in term for indicator in professional_indicators)
    
    def run_complete_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„é€æ–‡ä»¶åˆ†æ"""
        print("ğŸ” å¼€å§‹é€æ–‡ä»¶æœ¯è¯­åˆ†æ...")
        
        md_files = self.get_all_markdown_files()
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
        
        analysis_results = {
            'total_files': len(md_files),
            'analyzed_files': [],
            'missing_terms_summary': {},
            'domain_statistics': defaultdict(int)
        }
        
        for i, file_path in enumerate(md_files, 1):
            print(f"æ­£åœ¨åˆ†æ ({i}/{len(md_files)}): {file_path.name}")
            
            result = self.analyze_single_file(file_path)
            analysis_results['analyzed_files'].append(result)
            analysis_results['domain_statistics'][result['domain']] += 1
            
            if result.get('missing_count', 0) > 0:
                print(f"  âš ï¸  å‘ç° {result['missing_count']} ä¸ªç¼ºå¤±æœ¯è¯­")
        
        # æ±‡æ€»ç¼ºå¤±æœ¯è¯­
        analysis_results['missing_terms_summary'] = dict(self.missing_terms)
        
        print(f"\nğŸ“Š åˆ†æå®Œæˆ!")
        print(f"æ€»æ–‡ä»¶æ•°: {analysis_results['total_files']}")
        print(f"å‘ç°ç¼ºå¤±æœ¯è¯­ç§ç±»: {len(analysis_results['missing_terms_summary'])}")
        print(f"æ€»ç¼ºå¤±æœ¯è¯­å®ä¾‹: {sum(len(files) for files in self.missing_terms.values())}")
        
        return analysis_results
    
    def generate_detailed_report(self, analysis_results: Dict, output_file: str = "FILE_BY_FILE_TERMS_ANALYSIS.md"):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        report_content = "# é€æ–‡ä»¶æœ¯è¯­åˆ†ææŠ¥å‘Š\n\n"
        report_content += f"åˆ†ææ—¶é—´: {self.get_current_time()}\n\n"
        
        # æ€»ä½“ç»Ÿè®¡
        report_content += "## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n"
        report_content += f"- åˆ†ææ–‡ä»¶æ€»æ•°: {analysis_results['total_files']}\n"
        report_content += f"- å‘ç°ç¼ºå¤±æœ¯è¯­ç§ç±»: {len(analysis_results['missing_terms_summary'])}\n"
        report_content += f"- æ€»ç¼ºå¤±æœ¯è¯­å®ä¾‹: {sum(len(files) for files in analysis_results['missing_terms_summary'].values())}\n\n"
        
        # é¢†åŸŸåˆ†å¸ƒ
        report_content += "## ğŸ“š é¢†åŸŸåˆ†å¸ƒç»Ÿè®¡\n\n"
        report_content += "| é¢†åŸŸ | æ–‡ä»¶æ•°é‡ | å¹³å‡ç¼ºå¤±æœ¯è¯­æ•° |\n"
        report_content += "|------|----------|----------------|\n"
        
        for domain, count in analysis_results['domain_statistics'].items():
            if domain != 'error':
                domain_files = [f for f in analysis_results['analyzed_files'] if f['domain'] == domain]
                avg_missing = sum(f.get('missing_count', 0) for f in domain_files) / len(domain_files) if domain_files else 0
                report_content += f"| {domain} | {count} | {avg_missing:.1f} |\n"
        
        report_content += "\n"
        
        # ç¼ºå¤±æœ¯è¯­è¯¦æƒ…
        report_content += "## ğŸ” ç¼ºå¤±æœ¯è¯­è¯¦æƒ…\n\n"
        report_content += "| æœ¯è¯­ | å‡ºç°æ¬¡æ•° | é¦–æ¬¡å‡ºç°æ–‡ä»¶ | é¢†åŸŸ |\n"
        report_content += "|------|----------|----------------|------|\n"
        
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        sorted_missing = sorted(
            analysis_results['missing_terms_summary'].items(), 
            key=lambda x: len(x[1]), 
            reverse=True
        )
        
        for term, occurrences in sorted_missing[:100]:  # æ˜¾ç¤ºå‰100ä¸ª
            first_file = occurrences[0]['file']
            domain = occurrences[0]['domain']
            count = len(occurrences)
            report_content += f"| {term} | {count} | {first_file} | {domain} |\n"
        
        report_content += "\n"
        
        # æŒ‰æ–‡ä»¶çš„è¯¦ç»†åˆ†æ
        report_content += "## ğŸ“„ æ–‡ä»¶çº§åˆ«åˆ†æ\n\n"
        report_content += "| æ–‡ä»¶ | é¢†åŸŸ | ç¼ºå¤±æœ¯è¯­æ•° | ä¸»è¦ç¼ºå¤±æœ¯è¯­ |\n"
        report_content += "|------|------|------------|----------------|\n"
        
        for file_result in analysis_results['analyzed_files']:
            if file_result.get('missing_count', 0) > 0:
                missing_terms = file_result['missing_terms'][:5]  # æ˜¾ç¤ºå‰5ä¸ª
                main_terms = ', '.join(missing_terms)
                report_content += f"| {file_result['file']} | {file_result['domain']} | {file_result['missing_count']} | {main_terms} |\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.base_path / "tools" / output_file
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        print(f"ğŸ“„ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report_path
    
    def export_missing_terms_for_addition(self, analysis_results: Dict, output_file: str = "MISSING_TERMS_FOR_DICTIONARY.json"):
        """å¯¼å‡ºç¼ºå¤±æœ¯è¯­ç”¨äºè¯å…¸è¡¥å……"""
        missing_data = {
            'analysis_time': self.get_current_time(),
            'total_missing_terms': len(analysis_results['missing_terms_summary']),
            'terms_to_add': []
        }
        
        for term, occurrences in analysis_results['missing_terms_summary'].items():
            # è·å–ä»£è¡¨æ€§ä¿¡æ¯
            first_occurrence = occurrences[0]
            domains = list(set(occ['domain'] for occ in occurrences))
            
            missing_data['terms_to_add'].append({
                'term': term,
                'occurrence_count': len(occurrences),
                'domains': domains,
                'first_file': first_occurrence['file'],
                'sample_context': first_occurrence['context'][:100] + '...',
                'suggested_category': self._suggest_category(term, domains[0] if domains else 'general'),
                'definition_needed': True
            })
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_path = self.base_path / "tools" / output_file
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(missing_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ ç¼ºå¤±æœ¯è¯­æ•°æ®å·²å¯¼å‡ºåˆ°: {json_path}")
        return json_path
    
    def _suggest_category(self, term: str, domain: str) -> str:
        """ä¸ºæœ¯è¯­å»ºè®®åˆ†ç±»"""
        term_lower = term.lower()
        
        if 'ä½›æ•™' in term or 'ç¦…' in term or 'é“' in term:
            return 'ä¸œæ–¹ä¼ ç»Ÿæ™ºæ…§æœ¯è¯­'
        elif 'ç–—æ³•' in term or 'æ²»ç–—' in term:
            return 'æ²»ç–—æ–¹æ³•ä¸æŠ€æœ¯æœ¯è¯­'
        elif 'ç¥ç»' in term or 'å¤§è„‘' in term:
            return 'ç¥ç»ç§‘å­¦ä¸ç”Ÿç‰©åŒ»å­¦æœ¯è¯­'
        elif 'æµ‹è¯„' in term or 'é‡è¡¨' in term:
            return 'è¯„ä¼°ä¸æµ‹é‡æœ¯è¯­'
        elif 'å¿ƒç†å­¦' in term or 'è®¤çŸ¥' in term:
            return 'å¿ƒç†å­¦æ ¸å¿ƒæœ¯è¯­'
        else:
            return 'é€šç”¨æœ¯è¯­'
    
    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = FileByFileAnalyzer()
    results = analyzer.run_complete_analysis()
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_detailed_report(results)
    
    # å¯¼å‡ºç¼ºå¤±æœ¯è¯­
    analyzer.export_missing_terms_for_addition(results)
    
    print("\nâœ… é€æ–‡ä»¶æœ¯è¯­åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()